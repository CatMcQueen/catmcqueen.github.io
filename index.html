<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Use NLTK to Write Your Own Sherlock Novel</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="tutorial_lstm.tex"> 
<link rel="stylesheet" type="text/css" href="tutorial_lstm.css"> 
</head><body 
>
   <div class="maketitle">
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class="titleHead">Use NLTK to Write Your Own Sherlock Novel</h2>
<div class="author" ><span 
class="ptmr7t-x-x-120">Cat McQueen</span></div><br />
<div class="date" ><span 
class="ptmr7t-x-x-120">May 1, 2021</span></div>
   </div>
   <h3 class="likesectionHead"><a 
 id="x1-1000"></a>Contents</h3>
   <div class="tableofcontents">
   <span class="sectionToc" >1 <a 
href="#x1-20001" id="QQ2-1-2">Get Started on Google Colab</a></span>
<br />   <span class="sectionToc" >2 <a 
href="#x1-30002" id="QQ2-1-4">Import data</a></span>
<br />   &#x00A0;<span class="subsectionToc" >2.1 <a 
href="#x1-40002.1" id="QQ2-1-5">Method 1 (Temporary Upload):</a></span>
<br />   &#x00A0;<span class="subsectionToc" >2.2 <a 
href="#x1-50002.2" id="QQ2-1-8">Method 2 Google Drive:</a></span>
<br />   &#x00A0;<span class="subsectionToc" >2.3 <a 
href="#x1-60002.3" id="QQ2-1-9">Method 3 (Recommended):</a></span>
<br />   <span class="sectionToc" >3 <a 
href="#x1-70003" id="QQ2-1-10">Read and Process File</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.1 <a 
href="#x1-80003.1" id="QQ2-1-11">1: Removing White Space and Punctuation</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.2 <a 
href="#x1-90003.2" id="QQ2-1-12">Remove all Leading White Space:</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.3 <a 
href="#x1-100003.3" id="QQ2-1-13">Remove all Formatting Spaces:</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.4 <a 
href="#x1-110003.4" id="QQ2-1-14">Remove all Punctuation:</a></span>
<br />   <span class="sectionToc" >4 <a 
href="#x1-120004" id="QQ2-1-15">Analyze Corpus</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.1 <a 
href="#x1-130004.1" id="QQ2-1-16">Identify Word Based Data:</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.2 <a 
href="#x1-140004.2" id="QQ2-1-17">Identify Sentence Based Data:</a></span>
<br />   <span class="sectionToc" >5 <a 
href="#x1-150005" id="QQ2-1-18">Analyze Corpus</a></span>
<br />   &#x00A0;<span class="subsectionToc" >5.1 <a 
href="#x1-160005.1" id="QQ2-1-19">Data Preparation</a></span>
<br />   <span class="sectionToc" >6 <a 
href="#x1-170006" id="QQ2-1-20">Create and Run Model</a></span>
<br />   &#x00A0;<span class="subsectionToc" >6.1 <a 
href="#x1-180006.1" id="QQ2-1-21">Create Model</a></span>
<br />   &#x00A0;<span class="subsectionToc" >6.2 <a 
href="#x1-190006.2" id="QQ2-1-22">Writing Inline One-Hot Encoding</a></span>
<br />   &#x00A0;<span class="subsectionToc" >6.3 <a 
href="#x1-200006.3" id="QQ2-1-23">Speed Up Using Tensor GPU</a></span>
<br />   &#x00A0;<span class="subsectionToc" >6.4 <a 
href="#x1-210006.4" id="QQ2-1-25">Using Model Generate Text</a></span>
   </div>
                                                                  

                                                                  
<!--l. 30--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">1    </span> <a 
 id="x1-20001"></a>Get Started on Google Colab</h3>
<!--l. 32--><p class="noindent" >Google Colab is an environment that allows you to do Machine and Deep Learning for free. It
can be connected to GitHub and has GPUs that you can use for free. It runs as a Jupyter
notebook.
<!--l. 34--><p class="indent" >   Google Colab is found at https://colab.research.google.com and requires a gmail account.
Sign into your gmail account and it will open a pop-up. In the bottom right corner, select
&#8221;NEW NOTEBOOK&#8221;.
<!--l. 37--><p class="indent" >   The window should appear the same as Figure&#x00A0;<a 
href="#x1-2001r1">1<!--tex4ht:ref: fig:home --></a>
<!--l. 39--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
<a 
 id="x1-2001r1"></a>
                                                                  

                                                                  
<!--l. 41--><p class="noindent" ><img 
src="ColabHome.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;1: </span><span  
class="content">Colab Homepage</span></div><!--tex4ht:label?: x1-2001r1 -->
                                                                  

                                                                  
<!--l. 44--><p class="indent" >   </div><hr class="endfigure">
<!--l. 46--><p class="indent" >   Now that we&#8217;re on the right location, we can start coding!
   <h3 class="sectionHead"><span class="titlemark">2    </span> <a 
 id="x1-30002"></a>Import data</h3>
<!--l. 50--><p class="noindent" >The first thing we are going to do is import our Sherlock text file. Sherlock Holmes written by
Conan Doyle is now in the public domain and is a dataset we can use to train. This is
important to note: you cannot train networks for public consumption on texts that are
currently copyrighted.
<!--l. 52--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.1    </span> <a 
 id="x1-40002.1"></a>Method 1 (Temporary Upload):</h4>
<!--l. 54--><p class="noindent" >To download for this session only (the session will time out after some period of inactivity on
the page).
<!--l. 56--><p class="indent" >   First, download the data onto your local system by downloading <a 
href="https://github.com/CatMcQueen/catmcqueen.github.io/blob/b67a282c9be6bfd1ed17796c2507b9108cffb6bc/sherlock.txt" >the Sherlock Text</a>. On
the webpage, on the right, there is a Download button that will allow you to download this
text.
<!--l. 59--><p class="indent" >   Then, we are going to import it into our local Colab you have started. On the side of your
colab noteboook, you will see a file icon (the fourth icon down on the left bar), if you click on
it you will see a folder location. In order to connect to the folder location, you need to be
connected to the runtime, which requires you run something. To get the file folder connected,
type
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-1">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;print('hello&#x00A0;world')
</div>
<!--l. 62--><p class="nopar" >Then while still inside the text box, press shift and Enter. This will run the code within the
box you are currently in.
<!--l. 65--><p class="indent" >   Repeat every new Run time: You will then see an empty folder path. We are going to
upload sherlock.txt to our setup so we can call it in our python script.
<!--l. 68--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
<a 
 id="x1-4001r2"></a>
                                                                  

                                                                  
<!--l. 70--><p class="noindent" ><img 
src="FileFolder.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;2: </span><span  
class="content">The File Upload Bar</span></div><!--tex4ht:label?: x1-4001r2 -->
                                                                  

                                                                  
<!--l. 73--><p class="indent" >   </div><hr class="endfigure">
<!--l. 75--><p class="indent" >   Click on the upload icon, and upload the sherlock.txt you previously downloaded to your
local system. Now we&#8217;re ready to read it in and parse the data.
<!--l. 77--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
<a 
 id="x1-4002r3"></a>
                                                                  

                                                                  
<!--l. 79--><p class="noindent" ><img 
src="GoogleDriveConnect.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3: </span><span  
class="content">Select Connect to Google Drive</span></div><!--tex4ht:label?: x1-4002r3 -->
                                                                  

                                                                  
<!--l. 82--><p class="indent" >   </div><hr class="endfigure">
   <h4 class="subsectionHead"><span class="titlemark">2.2    </span> <a 
 id="x1-50002.2"></a>Method 2 Google Drive:</h4>
<!--l. 86--><p class="noindent" >First, download the data onto your local system by downloading <a 
href="https://github.com/CatMcQueen/catmcqueen.github.io/blob/b67a282c9be6bfd1ed17796c2507b9108cffb6bc/sherlock.txt" >the Sherlock Text</a>. On the
webpage, on the right, there is a Download button that will allow you to download this
text.
<!--l. 89--><p class="indent" >   To upload so the file can be read indefinitely, we&#8217;ll mount our google drive inline and
upload the file from there. First, click on the right Google Drive icon on the file
upload page shown in Figure&#x00A0;<a 
href="#x1-4001r2">2<!--tex4ht:ref: fig:filefolder --></a>. You will then see a window pop up asking you to
permit it to access your Drive. It may require you to register using an access code
the first time. When you see Figure&#x00A0;<a 
href="#x1-4002r3">3<!--tex4ht:ref: fig:connectdrive --></a> pop up, select Connect to Google Drive.
Run
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-2">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;%cd&#x00A0;drive/My&#x00A0;Drive/
</div>
<!--l. 93--><p class="nopar" >
<!--l. 95--><p class="indent" >   If you have saved sherlock.txt within another folder in Google Drive, append the folder
path to the path in the cd command. We will now be able to run our script even after the
runtime disconnects without loading sherlock.txt again.
<!--l. 97--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.3    </span> <a 
 id="x1-60002.3"></a>Method 3 (Recommended):</h4>
<!--l. 99--><p class="noindent" >We are going to do a git checkout from this repository of just the sherlock.txt. This link
can be found by going to <a 
href="https://github.com/CatMcQueen/catmcqueen.github.io/blob/main/sherlock.txt" >the Sherlock Text</a> and copying the permalink from that
webpage.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-3">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;!git&#x00A0;clone&#x00A0;-n&#x00A0;https://github.com/CatMcQueen/catmcqueen.github.io.git&#x00A0;--depth&#x00A0;1
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;%cd&#x00A0;catmcqueen.github.io
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;!git&#x00A0;checkout&#x00A0;HEAD&#x00A0;sherlock.txt
</div>
<!--l. 105--><p class="nopar" >
<!--l. 107--><p class="indent" >   This will checkout the file sherlock.txt from the repository, and allow us to call it in our
local system. Since it is part of our code, it will check it out every time we run
through the code; therefore, it does not require we reload data every runtime. It also
does not require you to ever download sherlock.txt to your own machine or Google
Drive.
<!--l. 109--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">3    </span> <a 
 id="x1-70003"></a>Read and Process File</h3>
<!--l. 111--><p class="noindent" >The first thing we are going to do is to read the text file in, and look at the data given. Now
that we have the sherlock.txt in our local runtime, read the file into a variable. Remember to
close the file after loading the data.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-4">
#&#x00A0;Open&#x00A0;a&#x00A0;file:&#x00A0;sherlock.txt&#x00A0;in&#x00A0;read&#x00A0;mode
&#x00A0;<br />file&#x00A0;=&#x00A0;open('sherlock.txt',mode='r')
&#x00A0;<br />
&#x00A0;<br />#&#x00A0;read&#x00A0;all&#x00A0;lines&#x00A0;into&#x00A0;sherlock_txt
&#x00A0;<br />sherlock_txt&#x00A0;=&#x00A0;file.read()
&#x00A0;<br />
&#x00A0;<br />#&#x00A0;close&#x00A0;the&#x00A0;file
&#x00A0;<br />file.close()
</div>
<!--l. 122--><p class="nopar" >
<!--l. 124--><p class="indent" >   Now that you have the file, we are going to process it. This entails 1) removing
punctuation and numbers, 2) removing any unnecessary white space, 3) reformatting into
sentences, instead of lines. Once the data is processed, we can tokenize it and begin to use the
data.
<!--l. 127--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.1    </span> <a 
 id="x1-80003.1"></a>1: Removing White Space and Punctuation</h4>
<!--l. 129--><p class="noindent" >Since we read the file in from text, there is a chance there is both leading white space(at the
beginning of the file) and empty lines. We read the file in all at once, so we are going to
process the text as a whole.
<!--l. 131--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.2    </span> <a 
 id="x1-90003.2"></a>Remove all Leading White Space:</h4>
<!--l. 133--><p class="noindent" >To remove the leading white space at the beginning of the file (we don&#8217;t want our first token
to be a space), we will run a regular expression to eliminate it.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-5">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;import&#x00A0;re
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;sherlock_text1&#x00A0;&#x00A0;=&#x00A0;re.sub('^\s+',&#x00A0;'',&#x00A0;sherlock_txt)
</div>
<!--l. 137--><p class="nopar" >
<!--l. 139--><p class="indent" >   re allows us to do a substitution, with re(x, y) replacing anything matching the x pattern
with y. The carat in regular expressions represents a new line, and the s is all spaces, with the
+ meaning that there has to be at least 1 for it to match. This regular expression, then, is
replacing anything number of spaces at the beginning of the file with an empty vector. In
practice, this removes all leading space characters.
<!--l. 141--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.3    </span> <a 
 id="x1-100003.3"></a>Remove all Formatting Spaces:</h4>
<!--l. 143--><p class="noindent" >Next we&#8217;ll remove all tabs, empty lines, and other forms of white space and replace them
with a single space. Since tokenizing often breaks on spaces, a single space between words is
the standard. In the same fashion as before, we&#8217;ll use re.sub.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-6">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;sherlock_text2&#x00A0;=&#x00A0;re.sub(r'\s+',&#x00A0;'&#x00A0;',&#x00A0;sherlock_text1)
</div>
<!--l. 146--><p class="nopar" >In this case, r&#8217; means to not use the backslash as a literal character, but to use it as part of the
spaces.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-7">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;'\s+'
&#x00A0;<br />&#x00A0;
</div>
<!--l. 150--><p class="nopar" >represents all whitespaces, so we are replacing all white spaces with a single space.
<!--l. 153--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.4    </span> <a 
 id="x1-110003.4"></a>Remove all Punctuation:</h4>
<!--l. 155--><p class="noindent" >When we are generating text, we will not use this; however, for analyzing what words are
most used and learning the data, we will want to remove punctuation, as they are the most
common &#8221;words&#8221; in our corpus, and can be considered stopwords (words too common in text
to be valuable to know) in that sense.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-8">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;sherlock_nopunc&#x00A0;=&#x00A0;re.sub('[^a-zA-Z&#x00A0;]',&#x00A0;'',&#x00A0;sherlock_text2)
&#x00A0;<br />&#x00A0;
</div>
<!--l. 158--><p class="nopar" >The carat in this set is a negator. Therefore, we are removing everything that is not also in the
brackets. [a-zA-Z] represents the set of all letters, lower and upper case. To make sure that we
are also retaining the spaces that we have inserted, there is also a space in the set. If we did
not include the space in this set, then all of the spaces would be removed from the text as
well.
<!--l. 161--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">4    </span> <a 
 id="x1-120004"></a>Analyze Corpus</h3>
<!--l. 164--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">4.1    </span> <a 
 id="x1-130004.1"></a>Identify Word Based Data:</h4>
<!--l. 166--><p class="noindent" >To do this we want to use our filtered dataset, tokenize it, and then do some analysis on what
our max used data is. To tokenize our document, we will use the word tokenize function from
NLTK.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-9">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;from&#x00A0;nltk.corpus&#x00A0;import&#x00A0;stopwords
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;from&#x00A0;nltk.tokenize&#x00A0;import&#x00A0;sent_tokenize,&#x00A0;word_tokenize
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;stopwords&#x00A0;=&#x00A0;set(stopwords.words('english'))
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;allwords&#x00A0;=&#x00A0;word_tokenize(sherlock_nopunc.lower())
&#x00A0;<br />&#x00A0;
</div>
<!--l. 172--><p class="nopar" >Once re have all of our words, we will want to remove all the stopwords (words that
are too common to be useful in learning about your dataset&#8211;think &#8217;the&#8217;, &#8217;a&#8217;, &#8217;and&#8217;,
etc).
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-10">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;clean_words&#x00A0;=&#x00A0;[k&#x00A0;for&#x00A0;k&#x00A0;in&#x00A0;allwords&#x00A0;if&#x00A0;k&#x00A0;not&#x00A0;in&#x00A0;stopwords]
&#x00A0;<br />&#x00A0;
</div>
<!--l. 176--><p class="nopar" >We here are taking only the words that are not in stopwords and putting them in a list. Now
we can analyze our list and get the most commonly used words. For right now, I only care
about the 20 most used words, so we are going to create a Counter (which creates a dictionary
of word, index pairs and gives context information about them. Counter has a function called
&#8221;most_common&#8221; that returns the dictionary item pairs of the x most commonly used
words.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-11">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;from&#x00A0;collections&#x00A0;import&#x00A0;Counter
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;def&#x00A0;most_frequent(inlist,&#x00A0;maxlist=10):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;occurence_count&#x00A0;&#x00A0;&#x00A0;&#x00A0;=&#x00A0;Counter(inlist)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;pairs&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;=&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;occurence_count.most_common(maxlist)#[0][0]
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;most_common&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;=&#x00A0;[]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;most_common_counts&#x00A0;=&#x00A0;[]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;for&#x00A0;x&#x00A0;in&#x00A0;range(maxlist):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;most_common.append(pairs[x][0])
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;most_common_counts.append(pairs[x][1])
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;return&#x00A0;most_common,&#x00A0;most_common_counts
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;top_words,&#x00A0;word_counts&#x00A0;=&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;most_frequent(clean_words)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;print(top_words)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;print(word_counts)
</div>
<!--l. 196--><p class="nopar" >When you run this cell, you should see two lists. The first is printing out the most used words,
and the second is printing out the number of times each of those is used. The output of mine
is:
<!--l. 200--><p class="indent" >   [&#8217;said&#8217;, &#8217;holmes&#8217;, &#8217;upon&#8217;, &#8217;one&#8217;, &#8217;would&#8217;, &#8217;man&#8217;, &#8217;could&#8217;, &#8217;mr&#8217;, &#8217;us&#8217;, &#8217;well&#8217;]
<!--l. 202--><p class="indent" >   [2734, 2386, 2302, 2217, 1900, 1826, 1639, 1389, 1340, 1268]
<!--l. 204--><p class="indent" >   These are the top 10 used words in our document. Interestingly, if you don&#8217;t force it to be
lowercase on allwords, the output is
<!--l. 206--><p class="indent" >   [&#8217;I&#8217;, &#8217;said&#8217;, &#8217;Holmes&#8217;, &#8217;The&#8217;, &#8217;upon&#8217;, &#8217;It&#8217;, &#8217;He&#8217;, &#8217;one&#8217;, &#8217;would&#8217;, &#8217;man&#8217;]
<!--l. 208--><p class="indent" >   [14361, 2734, 2383, 2380, 2288, 2122, 2062, 2033, 1864, 1807]
<!--l. 210--><p class="indent" >   As you can see, the stopwords are only in the lowercase, so filtering the list without
converting it to lower removes the filtering on the uppercase versions of the stop
words.
<!--l. 212--><p class="indent" >   Unsurprisingly, without filtering for stopwords, the output is
<!--l. 214--><p class="indent" >   [&#8217;the&#8217;, &#8217;and&#8217;, &#8217;of&#8217;, &#8217;I&#8217;, &#8217;to&#8217;, &#8217;a&#8217;, &#8217;that&#8217;, &#8217;in&#8217;, &#8217;was&#8217;, &#8217;it&#8217;]
<!--l. 216--><p class="indent" >   [29401, 14791, 14600, 14361, 13672, 13002, 9324, 8971, 8381, 7008]
<!--l. 218--><p class="indent" >   Now you can see why filtering by stop words is useful in learning your data.
<!--l. 220--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">4.2    </span> <a 
 id="x1-140004.2"></a>Identify Sentence Based Data:</h4>
<!--l. 222--><p class="noindent" >NLTK also has a sentence wise tokenizer, which breaks on common sentence tokens (&#8217;.&#8217;, &#8217;!&#8217;,
&#8217;?&#8217;, etc.) so let&#8217;s learn what the most common sentences are in our document.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-12">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;sentences&#x00A0;=&#x00A0;sent_tokenize(sherlock_text1.lower())
</div>
<!--l. 226--><p class="nopar" >
<!--l. 228--><p class="indent" >   Similar to how we analyzed our word count, we will now analyze which sentences are the
most common.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-13">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;from&#x00A0;collections&#x00A0;import&#x00A0;Counter
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;def&#x00A0;most_frequent(inlist,&#x00A0;maxlist=10):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;occurence_count&#x00A0;&#x00A0;&#x00A0;&#x00A0;=&#x00A0;Counter(inlist)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;pairs&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;=&#x00A0;occurence_count.most_common(maxlist)
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;most_common&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;=&#x00A0;[]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;most_common_counts&#x00A0;=&#x00A0;[]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;for&#x00A0;x&#x00A0;in&#x00A0;range(maxlist):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;most_common.append(pairs[x][0])
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;most_common_counts.append(pairs[x][1])
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;return&#x00A0;most_common,&#x00A0;most_common_counts
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;top_sent,&#x00A0;sent_counts&#x00A0;=&#x00A0;most_frequent(sentences,&#x00A0;maxlist=5)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;print(top_sent)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;print(sent_counts)
</div>
<!--l. 247--><p class="nopar" >
<!--l. 249--><p class="indent" >   running our sentence tokenizer, the output is
<!--l. 251--><p class="indent" >   [&#8217;i asked.&#8217;, &#8217;he asked.&#8217;, &#8217;he cried.&#8217;, &#8217;said he.&#8217;, &#8217;holmes?&#8221;&#8217;]
<!--l. 253--><p class="indent" >   [75, 59, 52, 48, 38]
<!--l. 255--><p class="indent" >   Meaning there were 75 sentences in our corpus that were &#8217;I asked.&#8217;
<!--l. 257--><p class="indent" >   Now that we know a little about our document, let&#8217;s train our model.
<!--l. 259--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">5    </span> <a 
 id="x1-150005"></a>Analyze Corpus</h3>
<!--l. 261--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.1    </span> <a 
 id="x1-160005.1"></a>Data Preparation</h4>
<!--l. 262--><p class="noindent" >For our data generation training, we don&#8217;t want to eliminate stopwords or punctuation.
Therefore, we will use sherlock_2 to train as we want there to be punctuation in our final
text.
<!--l. 264--><p class="indent" >   The first thing we&#8217;re going to do is reset allwords to be sherlock_2 and create a Counter
object for it. We will need that dictionary.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-14">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;Retokenize&#x00A0;words&#x00A0;for&#x00A0;word&#x00A0;generation
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;from&#x00A0;collections&#x00A0;import&#x00A0;Counter
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;allwords&#x00A0;=&#x00A0;word_tokenize(sherlock_text2.lower())
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;word_count&#x00A0;=&#x00A0;Counter(allwords)
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;VOCAB_SIZE&#x00A0;=&#x00A0;len(allwords)
&#x00A0;<br />&#x00A0;
</div>
<!--l. 272--><p class="nopar" >
<!--l. 274--><p class="indent" >   Now we have our new word list and dictionary, we are going to create a set of sequences.
Since Colab is RAM limited and the dataset is quite large, we are going to do a sequence of
series of words, using SEQ_LENGTH to determine how long the sequence is and STEP to
determine how far the steps are from each other. Here we&#8217;ll also set the number of epochs we
want to run, and the the number of hidden dimensions. Hidden dimensions can&#8217;t be too big or
the network will not work. This is a tunable parameter. Set these variables to allow the code
to work.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-15">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;first&#x00A0;set&#x00A0;the&#x00A0;dimensions
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;BATCH_SIZE&#x00A0;=&#x00A0;32
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;HIDDEN_DIM&#x00A0;=&#x00A0;50&#x00A0;#&#x00A0;tunable
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;SEQ_LENGTH&#x00A0;=&#x00A0;4
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;STEP&#x00A0;=&#x00A0;5
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;EPOCHS&#x00A0;=&#x00A0;1000
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;GENERATE_LENGTH&#x00A0;=&#x00A0;40&#x00A0;#&#x00A0;this&#x00A0;is&#x00A0;for&#x00A0;when&#x00A0;we&#x00A0;generate&#x00A0;text
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;LAYER_NUM&#x00A0;=&#x00A0;2&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;tunable&#x00A0;parameter
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;
</div>
<!--l. 287--><p class="nopar" >
<!--l. 290--><p class="indent" >   Now generate the sequences. We are using the dictionary listed out and tied to another
dictionary to map indexes of the dictionary to words.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-16">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;the&#x00A0;one&#x00A0;hot&#x00A0;is&#x00A0;too&#x00A0;expensive&#x00A0;to&#x00A0;do&#x00A0;in&#x00A0;words
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;def&#x00A0;make_sequences(text,&#x00A0;seq,&#x00A0;step,&#x00A0;txtdict):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;sequences&#x00A0;&#x00A0;=&#x00A0;[]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;cur_idx&#x00A0;&#x00A0;&#x00A0;&#x00A0;=&#x00A0;[]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;next_words&#x00A0;=&#x00A0;[]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;idx&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;=&#x00A0;[]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;txtdict_list&#x00A0;=&#x00A0;list(txtdict)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;create&#x00A0;a&#x00A0;dictionary&#x00A0;to&#x00A0;map&#x00A0;the&#x00A0;index&#x00A0;of&#x00A0;the&#x00A0;list&#x00A0;to&#x00A0;the&#x00A0;text&#x00A0;there
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;dictmap&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;=&#x00A0;{txtdict_list[i]:&#x00A0;i&#x00A0;&#x00A0;for&#x00A0;i&#x00A0;in&#x00A0;range(len(txtdict_list))}
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;skip&#x00A0;every&#x00A0;so&#x00A0;often,&#x00A0;and&#x00A0;collect&#x00A0;tokens
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;for&#x00A0;i&#x00A0;in&#x00A0;range(0,&#x00A0;VOCAB_SIZE&#x00A0;-&#x00A0;SEQ_LENGTH,&#x00A0;step):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;cur_words&#x00A0;=&#x00A0;text[i:&#x00A0;i&#x00A0;+&#x00A0;SEQ_LENGTH]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;sequences.append(cur_words)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;cur_seq&#x00A0;=&#x00A0;[]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;for&#x00A0;w&#x00A0;in&#x00A0;cur_words:
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;cur_seq.append(dictmap[w])
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;cur_idx.append(cur_seq)
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;next_word&#x00A0;=&#x00A0;text[i&#x00A0;+&#x00A0;SEQ_LENGTH]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;next_words.append(next_word)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;idx.append(dictmap[next_word])
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;return&#x00A0;sequences,&#x00A0;next_words,&#x00A0;cur_idx,&#x00A0;idx
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;too&#x00A0;big&#x00A0;of&#x00A0;a&#x00A0;dataset&#x00A0;to&#x00A0;do&#x00A0;all&#x00A0;of&#x00A0;them,&#x00A0;so&#x00A0;we're&#x00A0;training&#x00A0;on&#x00A0;a&#x00A0;subset,
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;splitting&#x00A0;using&#x00A0;sequence&#x00A0;length(&#x00A0;the&#x00A0;number&#x00A0;of&#x00A0;words&#x00A0;in&#x00A0;the&#x00A0;sequence)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;and&#x00A0;step&#x00A0;(the&#x00A0;number&#x00A0;of&#x00A0;units&#x00A0;skipped&#x00A0;between&#x00A0;each&#x00A0;step)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;seq,&#x00A0;next_words,&#x00A0;seq_idx,&#x00A0;next_idx&#x00A0;=&#x00A0;make_sequences(allwords,&#x00A0;SEQ_LENGTH,&#x00A0;STEP,&#x00A0;word_count)
&#x00A0;<br />
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;
</div>
<!--l. 322--><p class="nopar" >
<!--l. 324--><p class="indent" >   As you can see, we are returning a list of sequences, the next word, and then the index
for each. To run the model, this has to be one-hot encoded; however, since our
vocabulary is so large, we don&#8217;t have the RAM to do so. To fix this, we will later create
a data-loader to convert the label indexes into one-hot vectors for the model to
input.
<!--l. 326--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">6    </span> <a 
 id="x1-170006"></a>Create and Run Model</h3>
                                                                  

                                                                  
<!--l. 328--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.1    </span> <a 
 id="x1-180006.1"></a>Create Model</h4>
<!--l. 329--><p class="noindent" >We are going to use the Nadam optimizer, and the lr is a tunable parameter. Take time to
experiment with the learning rate to see what changes. We are doing a variable model that can
have extra LSTM layers in it. The LAYER_NUM variable allows you to change this number.
HIDDEN_DIM is the learnable dimensions, and is also tunable. Since we are on Google
Colab, that is restricted because of RAM size, but for models not learning, you can increase
or decrease this number.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-17">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;optimizer&#x00A0;=&#x00A0;optimizers.Nadam(lr=.001)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;def&#x00A0;create_model():
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;input_len&#x00A0;=&#x00A0;SEQ_LENGTH
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;model&#x00A0;=&#x00A0;Sequential()
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#since&#x00A0;we&#x00A0;know&#x00A0;the&#x00A0;input&#x00A0;size,&#x00A0;we&#x00A0;can&#x00A0;hard&#x00A0;code&#x00A0;this&#x00A0;input&#x00A0;shape
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;model.add(LSTM(HIDDEN_DIM,&#x00A0;input_shape=(SEQ_LENGTH,&#x00A0;VOCAB_SIZE)))
&#x00A0;<br />
&#x00A0;<br />
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;Add&#x00A0;Output&#x00A0;Layer
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;model.add(Dense(VOCAB_SIZE))
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;model.add(Activation('softmax'))
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;model.compile(loss='categorical_crossentropy',&#x00A0;optimizer=optimizer,&#x00A0;metrics=['mean_absolute_error',&#x00A0;"accuracy"])
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;return&#x00A0;model
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;model&#x00A0;=&#x00A0;create_model()
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;use&#x00A0;model.summary()&#x00A0;to&#x00A0;see&#x00A0;the&#x00A0;layers&#x00A0;and&#x00A0;their&#x00A0;respective&#x00A0;sizes
</div>
<!--l. 349--><p class="nopar" >
<!--l. 352--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.2    </span> <a 
 id="x1-190006.2"></a>Writing Inline One-Hot Encoding</h4>
<!--l. 354--><p class="noindent" >To do this, we need to replace the data keras generator to supply a new batch every
time with the one-hot batch. This way we don&#8217;t overwhelm the Colab notebook.
Based on <a 
href="https://github.com/keras-team/keras/issues/9707" >this Keras help page</a> we can get the format and necessary elements of the
Generator.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-18">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;class&#x00A0;Generator(Sequence):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;Class&#x00A0;is&#x00A0;a&#x00A0;dataset&#x00A0;wrapper&#x00A0;for&#x00A0;better&#x00A0;training&#x00A0;performance
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;def&#x00A0;__init__(self,&#x00A0;x_set,&#x00A0;y_set,&#x00A0;vocab_size,&#x00A0;batch_size=128):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;self.x,&#x00A0;self.y&#x00A0;=&#x00A0;x_set,&#x00A0;y_set
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;self.batch_size&#x00A0;=&#x00A0;batch_size
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;self.indices&#x00A0;=&#x00A0;np.arange(self.x.shape[0])&#x00A0;#&#x00A0;all&#x00A0;possible&#x00A0;sequences
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;self.vocab_size&#x00A0;=&#x00A0;vocab_size
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;def&#x00A0;__len__(self):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;return&#x00A0;math.ceil(self.x.shape[0]&#x00A0;/&#x00A0;self.batch_size)
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;def&#x00A0;__getitem__(self,&#x00A0;idx):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;inds&#x00A0;=&#x00A0;self.indices[idx&#x00A0;&#x22C6;&#x00A0;self.batch_size:(idx&#x00A0;+&#x00A0;1)&#x00A0;&#x22C6;&#x00A0;self.batch_size]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;indexed_x&#x00A0;=&#x00A0;self.x[inds]&#x00A0;#&#x00A0;sequences
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;indexed_y&#x00A0;=&#x00A0;self.y[inds]&#x00A0;#&#x00A0;next_words
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#print(f'indexed_x:&#x00A0;{indexed_x.shape},&#x00A0;indexed_y:&#x00A0;{indexed_y.shape}')
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;need&#x00A0;to&#x00A0;return&#x00A0;numpy&#x00A0;sparse&#x00A0;matrix&#x00A0;representation,&#x00A0;that's&#x00A0;why
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;we&#x00A0;need&#x00A0;to&#x00A0;do&#x00A0;batching,&#x00A0;so&#x00A0;use&#x00A0;bool&#x00A0;to&#x00A0;save&#x00A0;space&#x00A0;(we&#x00A0;don't&#x00A0;need&#x00A0;an&#x00A0;int)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;it's&#x00A0;one&#x00A0;hot&#x00A0;encoding&#x00A0;so&#x00A0;bool&#x00A0;works&#x00A0;fine
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;input&#x00A0;=&#x00A0;&#x00A0;[batch,&#x00A0;seq_length,&#x00A0;vocab_size]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;output&#x00A0;=&#x00A0;[batch,&#x00A0;1,&#x00A0;vocab_size]&#x00A0;but&#x00A0;we'll&#x00A0;smoosh&#x00A0;it
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;batch_x&#x00A0;=&#x00A0;np.zeros((self.batch_size,&#x00A0;self.x.shape[1],&#x00A0;self.vocab_size),&#x00A0;dtype=bool)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;batch_y&#x00A0;=&#x00A0;np.zeros((self.batch_size,&#x00A0;self.vocab_size),&#x00A0;dtype=np.bool)
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;for&#x00A0;batchidx,&#x00A0;bat&#x00A0;in&#x00A0;enumerate(indexed_y):&#x00A0;#&#x00A0;for&#x00A0;every&#x00A0;seq&#x00A0;in&#x00A0;the&#x00A0;batch
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;since&#x00A0;there's&#x00A0;only&#x00A0;on&#x00A0;item&#x00A0;in&#x00A0;each&#x00A0;list,&#x00A0;it's&#x00A0;just&#x00A0;a&#x00A0;list&#x00A0;of&#x00A0;vocab_size
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#print(batchidx,&#x00A0;bat)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;batch_y[batchidx][bat]&#x00A0;=&#x00A0;1.0
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#input_seq&#x00A0;=&#x00A0;np.zeros((self.y.shape[1],&#x00A0;VOCAB_SIZE))
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#for&#x00A0;listidx,&#x00A0;wordidx&#x00A0;in&#x00A0;enumerate(bat):&#x00A0;&#x00A0;#&#x00A0;for&#x00A0;each&#x00A0;item&#x00A0;in&#x00A0;the&#x00A0;seq
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;each&#x00A0;sequence&#x00A0;has&#x00A0;a&#x00A0;list&#x00A0;of&#x00A0;ints&#x00A0;(wordidx)&#x00A0;that&#x00A0;represents
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;the&#x00A0;total&#x00A0;index&#x00A0;into&#x00A0;the&#x00A0;vocabulary
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;&#x00A0;&#x00A0;&#x00A0;input_seq[listidx][wordidx]&#x00A0;=&#x00A0;1.0
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;&#x00A0;&#x00A0;&#x00A0;batch_x[batchidx]&#x00A0;=&#x00A0;input_sequence
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;for&#x00A0;batchidx,&#x00A0;bat&#x00A0;in&#x00A0;enumerate(indexed_x):&#x00A0;#&#x00A0;for&#x00A0;every&#x00A0;seq&#x00A0;in&#x00A0;the&#x00A0;batch
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;input_seq&#x00A0;=&#x00A0;np.zeros((self.x.shape[1],&#x00A0;VOCAB_SIZE))
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;for&#x00A0;listidx,&#x00A0;wordidx&#x00A0;in&#x00A0;enumerate(bat):&#x00A0;&#x00A0;#&#x00A0;for&#x00A0;each&#x00A0;item&#x00A0;in&#x00A0;the&#x00A0;seq
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;each&#x00A0;sequence&#x00A0;has&#x00A0;a&#x00A0;list&#x00A0;of&#x00A0;ints&#x00A0;(wordidx)&#x00A0;that&#x00A0;represents
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;the&#x00A0;total&#x00A0;index&#x00A0;into&#x00A0;the&#x00A0;vocabulary
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;input_seq[listidx][wordidx]&#x00A0;=&#x00A0;1.0
                                                                  

                                                                  
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;batch_x[batchidx]&#x00A0;=&#x00A0;input_seq
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;return&#x00A0;batch_x,&#x00A0;batch_y
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;def&#x00A0;on_epoch_end(self):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;just&#x00A0;shuffle&#x00A0;the&#x00A0;indexes&#x00A0;so&#x00A0;we&#x00A0;get&#x00A0;a&#x00A0;new&#x00A0;one&#x00A0;every&#x00A0;epoch
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;np.random.shuffle(self.indices)
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;train_generator&#x00A0;=&#x00A0;Generator(np.asarray(seq_idx),&#x00A0;np.asarray(next_idx),&#x00A0;VOCAB_SIZE,&#x00A0;batch_size=BATCH_SIZE)
</div>
<!--l. 407--><p class="nopar" >
<!--l. 409--><p class="indent" >   Now that our data is ready to put into the model, let&#8217;s call it by calling the generator class
and putting that as the input to the model.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-19">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;train_generator&#x00A0;=&#x00A0;Generator(np.asarray(seq_idx),&#x00A0;np.asarray(next_idx),&#x00A0;VOCAB_SIZE,&#x00A0;batch_size=BATCH_SIZE)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;model.fit(train_generator,&#x00A0;epochs=EPOCHS,&#x00A0;verbose=1,&#x00A0;batch_size=BATCH_SIZE)
</div>
<!--l. 414--><p class="nopar" >
<!--l. 416--><p class="indent" >   If that&#8217;s going really slow for you (likely). We can add a GPU to speed up the model.fit
process.
<!--l. 418--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.3    </span> <a 
 id="x1-200006.3"></a>Speed Up Using Tensor GPU</h4>
<!--l. 420--><p class="noindent" >To use the GPU to train (this will save you so much time), attach a GPU to your Colab. To do
this go to the Edit bar, and select Notebook Settings. From here you&#8217;ll select GPU to run your
network. TPUs require a lot of extra code to run, so make sure you select GPU, not
TPU.
<!--l. 422--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
<a 
 id="x1-20001r4"></a>
                                                                  

                                                                  
<!--l. 424--><p class="noindent" ><img 
src="GPUAccel.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;4: </span><span  
class="content">Use GPU to Accelerate Training</span></div><!--tex4ht:label?: x1-20001r4 -->
                                                                  

                                                                  
<!--l. 427--><p class="indent" >   </div><hr class="endfigure">
<!--l. 429--><p class="indent" >   Since GPU is already enabled in Tensorflow 2.0 (the default Tensorflow Google Colab
sources) you can run this like you would with a CPU. It does not require any changes to the
code.
   <h4 class="subsectionHead"><span class="titlemark">6.4    </span> <a 
 id="x1-210006.4"></a>Using Model Generate Text</h4>
<!--l. 432--><p class="noindent" >We have a trained model, now generate text from it. To do this we want more than just one
output word, so we are going to generate words in a function, and we are going to generate
GENERATE_LENGTH number of words. We&#8217;re going to pick a randomly generated integer
to get a word to start, then we are going to use the same random number to create a one-hot
array, so the input matches the expected. Since we are only doing this on one index at a time,
the space isn&#8217;t an issue. Doing that matches the expected input. From there, we will
append the words into a list, and then join them together to make a sentence. We
are using .join(&#8217; &#8217;) to force it to add a space between each item returned from the
list.
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-20">
&#x00A0;&#x00A0;&#x00A0;&#x00A0;#&#x00A0;method&#x00A0;for&#x00A0;generating&#x00A0;text
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;def&#x00A0;generate_text(model,&#x00A0;length,&#x00A0;&#x00A0;ix_to_word):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;ix&#x00A0;=&#x00A0;[np.random.randint(VOCAB_SIZE)]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;y_word&#x00A0;=&#x00A0;[ix_to_word[ix[-1]]]
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;X&#x00A0;=&#x00A0;np.zeros((1,&#x00A0;length,&#x00A0;VOCAB_SIZE))
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;for&#x00A0;i&#x00A0;in&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;range(length):
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;X[0,&#x00A0;i,&#x00A0;:][ix[-1]]&#x00A0;=&#x00A0;1
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;print(np.argmax(model.predict(X[:,&#x00A0;:i+1,&#x00A0;:])))
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;ix&#x00A0;=&#x00A0;np.argmax(model.predict(X[:,&#x00A0;:i+1,&#x00A0;:]),&#x00A0;1)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;print(ix_to_word[ix[-1]])
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;y_word.append(ix_to_word[ix[-1]])
&#x00A0;<br />
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;return&#x00A0;('&#x00A0;').join(y_word)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;txt&#x00A0;=&#x00A0;generate_text(model,&#x00A0;GENERATE_LENGTH,&#x00A0;list(word_count)
&#x00A0;<br />&#x00A0;&#x00A0;&#x00A0;&#x00A0;print(txt)
&#x00A0;<br />&#x00A0;
</div>
<!--l. 452--><p class="nopar" >
<!--l. 454--><p class="indent" >   This should write out generated text. Mine output as :
                                                                  

                                                                  
   <div class="verbatim" id="verbatim-21">
&#x00A0;xypquem
&#x00A0;<br />&#x00A0;
</div>
<!--l. 459--><p class="nopar" >
<!--l. 461--><p class="indent" >   Congratulations! You have now generated a document!
    
</body></html> 

                                                                  


