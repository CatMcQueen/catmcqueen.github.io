{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_onehot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjD5VQSNK2mUBTM3lJapth",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CatMcQueen/catmcqueen.github.io/blob/main/Tutorial_onehot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GztMqPq0HZkk"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#%cd drive/My Drive/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GaNdQ8gR-nh",
        "outputId": "9b2d237d-f9ab-43aa-c9d2-01d2ba923cfa"
      },
      "source": [
        "!git clone -n https://github.com/CatMcQueen/catmcqueen.github.io.git --depth 1\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'catmcqueen.github.io'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 12 (delta 2), reused 9 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (12/12), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68_VWl8-MEZJ",
        "outputId": "6a085f4e-ea88-4695-b7ab-c5cf2a9f5d48"
      },
      "source": [
        "#Then check out just the file you want like so:\n",
        "%cd catmcqueen.github.io\n",
        "!git checkout HEAD sherlock.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/catmcqueen.github.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uDmONGLpq5d",
        "outputId": "e45e485c-545d-41eb-8a5c-21834a61e21c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "stopwords = set(stopwords.words('english')) \n",
        "from collections import Counter"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdE2xR_GL2ro"
      },
      "source": [
        "file = open('sherlock.txt', mode='r')\n",
        "# read all lines in sherlock.txt into a list\n",
        "sherlock_txt = file.read()\n",
        " \n",
        "# make sure to always close the file\n",
        "file.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvv7-Dnvmf_i"
      },
      "source": [
        "## First break the text into sentences, removing numbers and unnecessary whitespaces.\n",
        "#sherlock_txt   = re.sub(r'[0-9]+', ' ', sherlock_txt) # do we want numbers? unclear\n",
        "sherlock_text1  = re.sub('^\\s+', '', sherlock_txt)# get rid of leading whitespace\n",
        "sherlock_text2  = re.sub('\\s+', ' ', sherlock_text1)# replace all tabs/spaces with spaces\n",
        "sherlock_nopunc = re.sub('[^a-zA-Z ]', '', sherlock_text2) # take out all punctuation\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTbysRHXphS7"
      },
      "source": [
        "# now that we are broken into sentences using the nltk sentence tolkenizer,\n",
        "# break each sentence into words\n",
        "\n",
        "sentences = sent_tokenize(sherlock_text1)\n",
        "#words    = [word_tokenize(t) for t in sentences]\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgro9ycfMrQw"
      },
      "source": [
        "\n",
        "allwords = word_tokenize(sherlock_nopunc.lower())"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8Sj419LVXrs"
      },
      "source": [
        "# make a list of sentences without the stopwords\n",
        "\n",
        "clean_sentences = []\n",
        "for sent in sentences:\n",
        "  splitsent = sent.split(' ')\n",
        "  clean_sentences.append([k for k in splitsent if k not in stopwords])\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6YHEEsHHLs_"
      },
      "source": [
        "#make a list of words without the stopwords\n",
        "clean_words = [k for k in allwords if k not in stopwords]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s16JFHx-IOLI",
        "outputId": "50b65041-3581-40bd-cdff-18ab80637d1d"
      },
      "source": [
        "\n",
        "from collections import Counter\n",
        "\n",
        "def most_frequent(inlist, maxlist=10):\n",
        "    occurence_count    = Counter(inlist)\n",
        "    pairs              = occurence_count.most_common(maxlist)#[0][0]\n",
        "\n",
        "    most_common        = []\n",
        "    most_common_counts = []\n",
        "    for x in range(maxlist):\n",
        "      most_common.append(pairs[x][0])\n",
        "      most_common_counts.append(pairs[x][1])\n",
        "    return most_common, most_common_counts\n",
        "\n",
        "top_words, word_counts = most_frequent(clean_words)\n",
        "print(top_words)\n",
        "print(word_counts)\n",
        "\n",
        "top_sent, sent_count  = most_frequent(sentences)\n",
        "print(top_sent)\n",
        "print(sent_count)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['said', 'holmes', 'upon', 'one', 'would', 'man', 'could', 'mr', 'us', 'well']\n",
            "[2734, 2386, 2302, 2217, 1900, 1826, 1639, 1389, 1340, 1268]\n",
            "['I asked.', 'he asked.', 'he cried.', 'said he.', 'Holmes?\"', '\"No.\"', '\"Exactly.', 'I cried.', '\"Yes.\"', 'said Holmes.']\n",
            "[71, 59, 48, 45, 38, 34, 33, 30, 30, 29]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV6rRKF9qTLd"
      },
      "source": [
        "## Text Generation\n",
        "We are doing world level LSTM, not character level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu2CD-L-qV3K"
      },
      "source": [
        "# import what we need to make the RNN/LSTM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "from keras.utils import Sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers  import  Embedding\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "import keras.utils as ku \n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from typing import Dict, List, Text, Union\n",
        "import math "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO5qCtWjqhDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d4d134-ada1-4f5a-d815-8349db0e8bff"
      },
      "source": [
        "# Retokenize words for word generation\n",
        "# get all of our words as tokens in a list\n",
        "allwords      = word_tokenize(sherlock_text2.lower())\n",
        "vocab         = set(allwords)\n",
        "VOCAB_SIZE    = len(vocab)\n",
        "\n",
        "print(VOCAB_SIZE)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yudCDYBcShRn"
      },
      "source": [
        "# first lets make our sequences\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# set it up so we can encode our data\n",
        "encoder      = LabelEncoder()\n",
        "values       = np.array(allwords)\n",
        "int_encoded  = encoder.fit_transform(values)\n",
        "\n",
        "# force it to be sparse until we need it to not in the Generator\n",
        "ohot_encoder = OneHotEncoder()\n",
        "\n",
        "#len(int_encoded) is the same length as our len(allwords)\n",
        "int_encoded  = int_encoded.reshape(len(int_encoded), 1) \n",
        "ohot_encoded = ohot_encoder.fit_transform(int_encoded)\n",
        "#verify it works\n",
        "idx          = int_encoded[0]\n",
        "inverted     = encoder.inverse_transform([np.argmax(ohot_encoded[idx, :])])\n",
        "# and to verify it works, we'll check that the inverted val is\n",
        "# the same as the index at that location\n",
        "assert allwords[idx[0]] == inverted[0]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MwQMeRCVTgY"
      },
      "source": [
        "# we have already read in our sherlock.txt and have the data loaded\n",
        "# now we can generate our own text\n",
        "SEQ_LENGTH = 4\n",
        "STEP = 5"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3D-Ad4GKG0y"
      },
      "source": [
        "# now we're going to make sequences of text, then following text and \n",
        "# one hot encode it\n",
        "# the one hot is too expensive to do in words\n",
        "\n",
        "def make_sequences(text, seq, step):\n",
        "        sequences  = []\n",
        "        next_words = []\n",
        "\n",
        "        # skip every so step, and collect tokens \n",
        "        for i in range(0, VOCAB_SIZE - seq, step):\n",
        "            cur_words = text[i: i + seq -1]\n",
        "            sequence = []\n",
        "            for idx, x in enumerate(cur_words):\n",
        "                one_hot = ohot_encoded[i + idx]\n",
        "                sequence.append(one_hot)\n",
        "            sequences.append(sequence)\n",
        "\n",
        "            next_word = text[i + seq]\n",
        "            one_hot = ohot_encoded[i + seq]\n",
        "            next_words.append(one_hot)\n",
        "        return sequences, next_words\n",
        "\n",
        "# too big of a dataset to do all of them, so we're training on a subset, \n",
        "# splitting using sequence length( the number of words in the sequence)\n",
        "# and step (the number of units skipped between each step)\n",
        "ohot_x, ohot_y = make_sequences(allwords, SEQ_LENGTH, STEP)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE6mLRAAIG4n"
      },
      "source": [
        "#set our model training values\n",
        "BATCH_SIZE = 1\n",
        "HIDDEN_DIM = 50 # may need to tune this. Check later\n",
        "EPOCHS = 30\n",
        "\n",
        "GENERATE_LENGTH = 20 # we shouldn't need more than 20 words in a sentence, that's a lot\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_QMOpPnH3Sg",
        "outputId": "d090c2f1-d159-4e00-9e7e-74b9f4e3261b"
      },
      "source": [
        "# now let's create our model\n",
        "optimizer = optimizers.Nadam(lr=.001)\n",
        "def create_model():\n",
        "    input_len = SEQ_LENGTH\n",
        "    model = Sequential()\n",
        "    #since we know the input size, we can hard code this input shape\n",
        "    model.add(LSTM(HIDDEN_DIM, input_shape=(SEQ_LENGTH, VOCAB_SIZE)))#, return_sequences=True))\n",
        "\n",
        "    model.add(Dense(VOCAB_SIZE))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['mean_absolute_error', \"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "# use model.summary() to see the layers and their respective sizes\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 50)                4013400   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 20016)             1020816   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 20016)             0         \n",
            "=================================================================\n",
            "Total params: 5,034,216\n",
            "Trainable params: 5,034,216\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB1kyZ6rPuuG"
      },
      "source": [
        "## https://github.com/keras-team/keras/issues/9707 says this is how to force\n",
        "## keras to shuffle data every time manually\n",
        "## Keras has a native shuffler, but since our output isn't one hot, we need\n",
        "## to shuffle the data to ensure it does get shuffled. \n",
        "class Generator(Sequence):\n",
        "    # Class is a dataset wrapper for better training performance\n",
        "    def __init__(self, x_set, y_set, vocab_size, batch_size=128):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(self.x.shape[0]) # all possible sequences\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(self.x.shape[0] / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        indexed_x = self.x[inds] # sequences\n",
        "        indexed_y = self.y[inds] # next_words\n",
        "\n",
        "        # we need to do batching, so use bool to save space (we don't need an int)\n",
        "        # it's one hot encoding so bool works fine\n",
        "        # x =  [batch, seq_length, vocab_size]\n",
        "        # y = [batch, vocab_size] \n",
        "        # for y it's just an array of sparse matricies, so just convert it\n",
        "        batch_y = np.asarray(indexed_y[0].todense())\n",
        "\n",
        "        # for x, it's a little harder, since it's an array of 3 dense matricies\n",
        "        # first convert it to a list, then convert it to a dense matrix, then \n",
        "        # convert it back into numpy array\n",
        "        batch_x = []\n",
        "        for batchidx, bat in enumerate(indexed_x): # for every seq in the batch\n",
        "            batch_x.append(bat[0].todense())\n",
        "        batch_x = np.array(batch_x) \n",
        "\n",
        "        return batch_x, batch_y\n",
        "        \n",
        "        return batch_x, batch_y\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "train_generator = Generator(np.array(ohot_x), np.array(ohot_y), VOCAB_SIZE, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YViyKxTPaX_F",
        "outputId": "0084b271-d1a1-485a-b5a3-f3a95790ec2a"
      },
      "source": [
        "#Keras fit has an inline batch sizer with defaulted shuffle = true, so this should shuffle\n",
        "# the data without taking too much in at once\n",
        "model.fit(train_generator, epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 6.2988 - mean_absolute_error: 9.8746e-05 - accuracy: 0.0600\n",
            "Epoch 2/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 5.8042 - mean_absolute_error: 9.8534e-05 - accuracy: 0.0597\n",
            "Epoch 3/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 5.6233 - mean_absolute_error: 9.8342e-05 - accuracy: 0.0615\n",
            "Epoch 4/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 5.5163 - mean_absolute_error: 9.8137e-05 - accuracy: 0.0615\n",
            "Epoch 5/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 5.4254 - mean_absolute_error: 9.8026e-05 - accuracy: 0.0622\n",
            "Epoch 6/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 5.3026 - mean_absolute_error: 9.7775e-05 - accuracy: 0.0697\n",
            "Epoch 7/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 5.1452 - mean_absolute_error: 9.7354e-05 - accuracy: 0.0764\n",
            "Epoch 8/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 4.9535 - mean_absolute_error: 9.6803e-05 - accuracy: 0.0899\n",
            "Epoch 9/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 4.7306 - mean_absolute_error: 9.5891e-05 - accuracy: 0.1212\n",
            "Epoch 10/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 4.4903 - mean_absolute_error: 9.4361e-05 - accuracy: 0.1794\n",
            "Epoch 11/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 4.2471 - mean_absolute_error: 9.2186e-05 - accuracy: 0.2201\n",
            "Epoch 12/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 4.0259 - mean_absolute_error: 8.9762e-05 - accuracy: 0.2506\n",
            "Epoch 13/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 3.8273 - mean_absolute_error: 8.7523e-05 - accuracy: 0.2750\n",
            "Epoch 14/30\n",
            "4003/4003 [==============================] - 58s 15ms/step - loss: 3.6551 - mean_absolute_error: 8.5549e-05 - accuracy: 0.2820\n",
            "Epoch 15/30\n",
            "4003/4003 [==============================] - 58s 15ms/step - loss: 3.5124 - mean_absolute_error: 8.3919e-05 - accuracy: 0.2890\n",
            "Epoch 16/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 3.3904 - mean_absolute_error: 8.2272e-05 - accuracy: 0.2948\n",
            "Epoch 17/30\n",
            "4003/4003 [==============================] - 58s 15ms/step - loss: 3.2864 - mean_absolute_error: 8.0843e-05 - accuracy: 0.2958\n",
            "Epoch 18/30\n",
            "4003/4003 [==============================] - 58s 15ms/step - loss: 3.2066 - mean_absolute_error: 7.9749e-05 - accuracy: 0.2993\n",
            "Epoch 19/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 3.1446 - mean_absolute_error: 7.8658e-05 - accuracy: 0.2955\n",
            "Epoch 20/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 3.0922 - mean_absolute_error: 7.7994e-05 - accuracy: 0.2983\n",
            "Epoch 21/30\n",
            "4003/4003 [==============================] - 58s 15ms/step - loss: 3.0459 - mean_absolute_error: 7.7213e-05 - accuracy: 0.2940\n",
            "Epoch 22/30\n",
            "4003/4003 [==============================] - 58s 15ms/step - loss: 3.0094 - mean_absolute_error: 7.6763e-05 - accuracy: 0.2895\n",
            "Epoch 23/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 2.9873 - mean_absolute_error: 7.6381e-05 - accuracy: 0.2843\n",
            "Epoch 24/30\n",
            "4003/4003 [==============================] - 58s 15ms/step - loss: 2.9675 - mean_absolute_error: 7.6028e-05 - accuracy: 0.2825\n",
            "Epoch 25/30\n",
            "4003/4003 [==============================] - 58s 15ms/step - loss: 2.9493 - mean_absolute_error: 7.5672e-05 - accuracy: 0.2895\n",
            "Epoch 26/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 2.9349 - mean_absolute_error: 7.5436e-05 - accuracy: 0.2878\n",
            "Epoch 27/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 2.9184 - mean_absolute_error: 7.5256e-05 - accuracy: 0.2835\n",
            "Epoch 28/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 2.9111 - mean_absolute_error: 7.5142e-05 - accuracy: 0.2860\n",
            "Epoch 29/30\n",
            "4003/4003 [==============================] - 59s 15ms/step - loss: 2.9032 - mean_absolute_error: 7.4948e-05 - accuracy: 0.2860\n",
            "Epoch 30/30\n",
            "4003/4003 [==============================] - 58s 15ms/step - loss: 2.8979 - mean_absolute_error: 7.4894e-05 - accuracy: 0.2830\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbdaf62cc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWbRan0GOQcg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "43fd72f4-29cf-4ca7-cd64-87fbf881bd72"
      },
      "source": [
        "# now we use that to generate text\n",
        "# since we need to input items that are one-hot encoded, we're going\n",
        "# to use the same method as before to turn it into a one-hot encoding \n",
        "# method for generating text\n",
        "def generate_text(model, length, text_size):\n",
        "    # first get a random word as our starting point (input)\n",
        "    ix     = [np.random.randint(text_size-SEQ_LENGTH) + SEQ_LENGTH]\n",
        "    val    = [np.argmax(ohot_encoded[idx, :])]\n",
        "    y_char = np.ndarray.tolist(encoder.inverse_transform(val))\n",
        "\n",
        "    # this is the output text, it'll be length long\n",
        "    for i in range(length):\n",
        "        #try:\n",
        "        # one hot encode the current word\n",
        "        X_words = []\n",
        "        for n in range(SEQ_LENGTH):\n",
        "          indexer = ix[0] - SEQ_LENGTH + n\n",
        "          X = ohot_encoded[indexer, :]\n",
        "          X = X.todense()\n",
        "          X_words.append(X)\n",
        "        X_words = np.array(X_words)\n",
        "        #then the index changes to the predicted word and we continue\n",
        "        ix = [np.argmax(model.predict(X_words, 1))]\n",
        "        val = [np.argmax(ohot_encoded[ix, :])]\n",
        "        y_char.append(np.ndarray.tolist(encoder.inverse_transform(val))[0])\n",
        "        #except:\n",
        "        #    continue\n",
        "\n",
        "    # then combine all the words into a sentence        \n",
        "    return (' ').join(y_char)\n",
        "\n",
        "generate_text(model, GENERATE_LENGTH, len(allwords))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'must he mixed gregson pass to here said and her i of in and still redskins horsemen falling to for refer'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}